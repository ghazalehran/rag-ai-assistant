
# üß† AI Research Assistant ‚Äî RAG-based QA for Scientific PDFs

This is a modular **Retrieval-Augmented Generation (RAG)** app built with **LangChain**, **FAISS/Chroma**, and **OpenAI/Hugging Face** models. It enables you to upload research PDFs and ask intelligent, context-aware questions ‚Äî powered by embeddings, vector search, and LLMs.

> ‚öôÔ∏è Designed to be scalable, extensible, and cloud-deployable (Hugging Face, Streamlit). Ideal for demonstrating production-grade NLP capabilities.

---

## üì∏ Live Demo Interface

<img src="static\gradio-interface.png" alt="Gradio UI Screenshot" width="100%">

---

## üöÄ Key Features

- ‚úÖ Upload any PDF research paper
- ‚úÇÔ∏è Smart chunking & vector embedding
- üîé Search via **FAISS** (default) or **ChromaDB**
- üß† Flexible LLM backends: `OpenAI`, `Hugging Face`, or your own
- üîç Dual QA strategy: strict context-only + fallback summarization
- üíæ Cached vectorstores ‚Äî no reprocessing
- üß™ Dev mode for testing without LLM cost
- üåê Gradio interface included for rapid UI

---

## üõ†Ô∏è Skills & Stack Highlights

- **Python**, **LangChain**, **LLMs**, **RAG**
- OpenAI API, Hugging Face Hub models
- FAISS, ChromaDB, PyPDFLoader
- Gradio UI, .env + Hugging Face secrets
- Local + cloud deployability

---

## üìÅ Folder Structure

```
rag-ai-assistant/
‚îú‚îÄ‚îÄ app_local.py           # Gradio UI for local testing
‚îú‚îÄ‚îÄ main.py                # Optional CLI/deployment entry point
‚îú‚îÄ‚îÄ loader.py              # Vectorstore creator/loader
‚îú‚îÄ‚îÄ utils.py               # QA chain logic with fallback
‚îú‚îÄ‚îÄ llm_provider.py        # LLM loader (OpenAI / HF)
‚îú‚îÄ‚îÄ config.py              # Centralized config for env + models
‚îú‚îÄ‚îÄ documents/             # Drop your PDFs here
‚îú‚îÄ‚îÄ faiss_dbs/             # Vectorstore (FAISS)
‚îú‚îÄ‚îÄ chroma_dbs/            # Vectorstore (Chroma)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env                   # Only used locally
```

---

## ‚öôÔ∏è Quick Setup

### 1. Clone & Set Up

```bash
git clone https://github.com/ghazalehran/rag-ai-assistant.git
cd rag-ai-assistant
python -m venv rag-env
source rag-env/bin/activate  # Windows: rag-env\Scripts\activate
pip install -r requirements.txt
```

### 2. Environment Variables

Create a `.env` file (used locally only):

```env
OPENAI_API_KEY=sk-...
HUGGINGFACEHUB_API_TOKEN=hf-...
```

> For Hugging Face or Streamlit cloud deploy, use **secret management UI**.

---

## üñ•Ô∏è Run the Gradio App

```bash
python app_local.py
```

This opens a local Gradio interface in your browser where you can:

- Upload a PDF
- Ask a natural language question
- Get an intelligent answer from the LLM

---

## üîÑ Configurable Options

All configurations are managed via `config.py`:

| Option               | Description                                |
|----------------------|--------------------------------------------|
| `LLM_BACKEND`        | `"openai"` / `"huggingface"`               |
| `VECTORSTORE_BACKEND`| `"faiss"` / `"chroma"`                     |
| `CHUNK_SIZE`         | Default: `800` tokens                      |
| `TEMPERATURE`        | LLM creativity (default: `0.2`)            |
| `USE_HF_SECRETS`     | `True` if deploying on Hugging Face Cloud  |

---

## ‚úÖ Example Models Used

- **OpenAI**: `gpt-4`, `gpt-3.5-turbo`
- **Hugging Face**: [`mistralai/Mistral-7B-Instruct-v0.2`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

---

## üìÑ License

MIT ‚Äî free for personal, academic, or professional use.

---

## üôã‚Äç‚ôÇÔ∏è Questions?

Feel free to open an issue or connect on [LinkedIn](https://linkedin.com).

---