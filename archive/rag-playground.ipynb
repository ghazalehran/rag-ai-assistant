{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"documents/TWDpdf.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#print(docs[0].page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 102\n",
      "\n",
      "üß© First chunk preview:\n",
      "1 \n",
      "Towards Multi-Brain Decoding in Autism:  \n",
      "A Self-Supervised Learning Approach \n",
      " \n",
      "Ghazaleh Ranjabaran1, Quentin Moreau1, Adrien Dubois1, Guillaume \n",
      "Dumas*1,2, \n",
      " \n",
      "1CHU Sainte-Justine Research Centre, Department of Psychiatry, Universit√© de Montr√©al, \n",
      "Montr√©al, QC, Canada \n",
      "2Mila ‚Äì Quebec AI Institut...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,         # size of each chunk\n",
    "    chunk_overlap=100       # how much each chunk overlaps with the previous one\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Inspect the result\n",
    "print(f\"‚úÖ Total chunks created: {len(split_docs)}\")\n",
    "print(f\"\\nüß© First chunk preview:\\n{split_docs[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\")[:8])  # Should show \"sk-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# (You won‚Äôt see any output yet ‚Äî embeddings will be used in the next step)\n",
    "print(\"‚úÖ Embedding model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromaDB is a lightweight, local vector database ‚Äî it's used to:\n",
    "\n",
    "- Store text embeddings (numeric vectors)\n",
    "\n",
    "- Search for similar vectors based on user queries\n",
    "\n",
    "- Power semantic search in RAG systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings stored in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#Create Chroma DB and store the embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "# Save the DB to disk\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"‚úÖ Embeddings stored in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Result 1:\n",
      "the purpose of acquiring meaningful representations from EEG data. They presented two specific \n",
      "self-supervised learning (SSL) tasks, namely relative positioning (RP) and temporal shuffling (TS) \n",
      "and adapted a third technique called contrastive predictive coding (CPC) (Oord et al., 2019) to be \n",
      "applicable to EEG data. \n",
      " \n",
      "This study aims to comprehensively analyze hyperscanning EEG data obtained from both autistic \n",
      "and neurotypical participants. The primary objective is to develop a DL model expl...\n",
      "\n",
      "\n",
      "üîé Result 2:\n",
      "downstream tasks. In the initial pretext task phase, the model is presented with a set of self -\n",
      "generated challe nges or auxiliary objectives. These challenges are carefully designed to \n",
      "encourage the model to extract meaningful and informative features from the unlabeled data. The \n",
      "model learns to uncover patterns, relationships, and representations within the data it self, \n",
      "effectively transforming it into a more structured and informative format....\n",
      "\n",
      "\n",
      "üîé Result 3:\n",
      "downstream tasks. In the initial pretext task phase, the model is presented with a set of self -\n",
      "generated challe nges or auxiliary objectives. These challenges are carefully designed to \n",
      "encourage the model to extract meaningful and informative features from the unlabeled data. The \n",
      "model learns to uncover patterns, relationships, and representations within the data it self, \n",
      "effectively transforming it into a more structured and informative format....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main goal of the research?\"\n",
    "results = vectorstore.similarity_search(query, k=3)  # return top 3 relevant chunks\n",
    "\n",
    "# Display the results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nüîé Result {i+1}:\\n{res.page_content[:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "the purpose of acquiring meaningful representations from EEG data. They presented two specific \n",
      "self-supervised learning (SSL) tasks, namely relative positioning (RP) and temporal shuffling (TS) \n",
      "and adapted a third technique called contrastive predictive coding (CPC) (Oord et al., 2019) to be \n",
      "appl...\n",
      "\n",
      "Chunk 2:\n",
      "methods. This research also emphasizes the broader role of computational models in precision \n",
      "psychiatry, paving the way for innovative, personalized diagnostic and therapeutic solutions. \n",
      " \n",
      "References  \n",
      " \n",
      "Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A Next-generation \n",
      "Hyp...\n",
      "\n",
      "Chunk 3:\n",
      "and further analyses of this parameter are essential. Specifically, investigating the upper bound \n",
      "for positive context length and comparing it to the total duration of EEG recordings could offer \n",
      "valuable insights into optimizing the pretext task model for this dataset and task....\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the goal of this research?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nChunk {i+1}:\\n{doc.page_content[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",  # or \"gpt3\" \n",
    "    temperature=0.1,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized promp - strict answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "template = template = \"\"\"\n",
    "You are a helpful assistant analyzing a scientific research paper.\n",
    "\n",
    "Use only the context provided below to answer the question. Do not rely on outside knowledge.\n",
    "\n",
    "- If the answer is clearly present, respond with a concise and accurate explanation.\n",
    "- If the exact term used in the question does not appear in the text, but an equivalent or related term is used instead, mention and use that term in your answer.\n",
    "- If the answer is not present at all in the context, respond with: \"Not found in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      " The goal of this research is to comprehensively analyze hyperscanning EEG data obtained from both autistic and neurotypical participants. The primary objective is to develop a deep learning model explicitly designed to extract and recognize patterns and relationships within individual EEG signals, using a self-supervised learning methodology. This research also aims to emphasize the broader role of computational models in precision psychiatry, paving the way for innovative, personalized diagnostic and therapeutic solutions.\n"
     ]
    }
   ],
   "source": [
    "# Run it\n",
    "answer = qa_chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(\"\\nüß† Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default setting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      " The goal of this research is to comprehensively analyze hyperscanning EEG data obtained from both autistic and neurotypical participants. The primary objective is to develop a deep learning model specifically designed to extract and recognize patterns and relationships within individual EEG signals, using a self-supervised learning methodology. This research also aims to emphasize the broader role of computational models in precision psychiatry, potentially paving the way for innovative, personalized diagnostic and therapeutic solutions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# \"stuff\" chain just stuffs the documents into the prompt\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Run it\n",
    "answer = qa_chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(\"\\nüß† Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Fallback -  fallback QA is a common pattern in production RAG systems. It‚Äôs a backup strategy used when retrieval fails, meaning:\n",
    "\n",
    "If standard RAG fails, run GPT on the full doc intro + conclusion (with a warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Fallback prompt for when retrieval fails\n",
    "fallback_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant summarizing a scientific research paper.\n",
    "\n",
    "The user has a question about the paper. Use the context below to answer it as clearly and accurately as possible. \n",
    "If the exact answer is not stated, provide a useful summary based on what you understand from the content.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "fallback_chain = LLMChain(llm=llm, prompt=fallback_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing final responses for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîπ Question 1: What is the main goal of the research?\n",
      "üß† Final Answer:\n",
      " The main goal of the research is \"to comprehensively analyze hyperscanning EEG data obtained from both autistic and neurotypical participants.\" The primary objective is \"to develop a DL model explicitly designed to extract and recognize patterns and relationships within individual EEG signals, employing a self-supervised learning methodology.\"\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The main goal of the research is to comprehensively analyze hyperscanning EEG data obtained from both autistic and neurotypical participants. The primary objective is to develop a deep learning model specifically designed to extract and recognize patterns and relationships within individual EEG signals, using a self-supervised learning methodology.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 2: What is the main hypothesis or research question?\n",
      "‚ö†Ô∏è Using fallback context...\n",
      "üß† Final Answer:\n",
      " The main research question is not explicitly stated in the provided context. However, based on the information given, the research appears to be focused on exploring the effectiveness of self-supervised learning (SSL) in classifying hyperscanning-EEG data, specifically in distinguishing between autistic and neurotypical individuals. The study also seems to be interested in understanding the relationship between model complexity, architecture, and dataset size in this context.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The text does not provide information on the main hypothesis or research question of the study.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 3: Summarize the findings of this study.\n",
      "‚ö†Ô∏è Using fallback context...\n",
      "üß† Final Answer:\n",
      " The study developed a multi-brain classification model using a large-scale, single-brain EEG dataset for self-supervised learning (SSL) pretraining. This model was then fine-tuned with hyperscanning data from interactions involving Autism Spectrum Condition (ASC) and neurotypical participants. The SSL model demonstrated superior performance (78.13% accuracy) compared to supervised baselines and logistic regression using spectral EEG biomarkers. This suggests that SSL can effectively address the challenges of limited labeled data, enhance EEG-based diagnostic tools for ASC, and advance research in social neuroscience. The study underscores the potential of SSL in improving diagnostic tools for autism.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " I'm sorry, but the provided context does not include specific findings of the study. It only mentions the limitations of the study and a need for deeper investigation.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 4: What are the main conclusions drawn in the paper?\n",
      "‚ö†Ô∏è Using fallback context...\n",
      "üß† Final Answer:\n",
      " The main conclusions drawn in the paper are that self-supervised learning (SSL) is effective for classifying hyperscanning-EEG data, specifically in distinguishing between autistic and neurotypical individuals. The SSL multi-brain model significantly outperformed supervised and traditional machine learning baselines, demonstrating its ability to extract meaningful patterns from limited labeled data. The study underscores the potential of SSL to enhance diagnostic tools for autism and advance research in social neuroscience.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The text doesn't provide information about the main conclusions drawn in the paper.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 5: What data or participants were used in the study?\n",
      "üß† Final Answer:\n",
      " The study used a subset of the HBN dataset, consisting of 1,000 participants, which is approximately one-third of the available data. For the downstream task, the BBC2 dataset was used, which includes hyperscanning EEG data from 36 participants in dyads: 9 mixed (TD-ASC) and 9 control (TD-TD).\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The study used two datasets. The models were trained on a subset of the HBN dataset, which consisted of 1,000 participants, approximately one-third of the available data. For the downstream task, the BBC2 dataset was used, which includes hyperscanning EEG data from 36 participants in dyads: 9 mixed (TD-ASC) and 9 control (TD-TD).\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 6: What methods or models were applied?\n",
      "üß† Final Answer:\n",
      " The methods or models applied were: a supervised multi-brain model, a logistic regression model, and a purely supervised model. The logistic regression classifier used extracted spectral EEG biomarkers.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The methods or models that were applied include a supervised multi-brain model and a logistic regression classifier using extracted spectral EEG biomarkers. These are referred to as baseline approaches. The supervised model does not involve any pretext training, meaning it starts from scratch.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 7: What is Self-Supervised Learning in the context of this research?\n",
      "üß† Final Answer:\n",
      " Self-supervised learning, in the context of this research, is a dual-stage process that includes two main components: the pretext task and the downstream task. Pretext tasks are the foundational building blocks of self-supervised learning, forming a bridge between raw data and the goal of training a model for downstream tasks. In the initial pretext task phase, the model is presented with a set of self-generated challenges or auxiliary objectives, designed to encourage the model to extract meaningful and informative features from the unlabeled data. The model learns to uncover patterns, relationships, and representations within the data itself, effectively transforming it into a more structured and informative format.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " Self-supervised learning, in the context of this research, is a dual-stage process that includes two main components: the pretext task and the downstream task. The pretext tasks are the foundational building blocks of self-supervised learning, serving as a bridge between raw data and the goal of training a model for downstream tasks. In the pretext task phase, the model is presented with a set of self-generated challenges or auxiliary objectives, which are designed to encourage the model to extract meaningful and informative features from the unlabeled data. The model learns to uncover patterns, relationships, and representations within the data itself, effectively transforming it into a more structured and informative format.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 8: Did the SSL model outperform the baseline?\n",
      "üß† Final Answer:\n",
      " Yes, the SSL model did outperform the baseline. The text states, \"These findings demonstrate the superiority of the SSL Multi -Brain model, which not only outperforms traditional and supervised deep learning models but also handles the class imbalance effectively.\"\n",
      "\n",
      "üß† Flexible Answer:\n",
      " Yes, the SSL Multi-Brain model outperformed the baseline models.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 9: How is this research relevant to autism?\n",
      "‚ö†Ô∏è Using fallback context...\n",
      "üß† Final Answer:\n",
      " This research is relevant to autism as it develops a new method for diagnosing Autism Spectrum Condition (ASC) using self-supervised learning (SSL) and EEG data. The study leverages a large-scale, single-brain EEG dataset to create a multi-brain classification model that can distinguish between autistic and neurotypical individuals. This model demonstrated superior performance compared to traditional methods, suggesting its potential to enhance diagnostic tools for autism. This is particularly important as diagnosing ASC in adults presents challenges due to a lack of validated tools. Therefore, this research not only advances the understanding of neural mechanisms underlying ASC but also contributes to the development of more effective diagnostic tools.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The research mentioned in the context is relevant to autism in several ways. The first study discusses the rehabilitation of Autism Spectrum Disorder, indicating a focus on treatment strategies. The second study by Kruppa et al. examines brain and motor synchrony in children and adolescents with Autism Spectrum Disorder (ASD), which could provide insights into the neurological aspects of ASD. The third study by Leong et al. discusses how speaker gaze increases information coupling between infant and adult brains, which could have implications for understanding and improving communication in individuals with autism. The final study by Bottema-Beutel, Kim, and Crowley presents a systematic review and meta-regression analysis of social functioning correlates in autism and typical development, which could help identify key factors affecting social functioning in individuals with autism.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "================================================================================\n",
      "üîπ Question 10: What limitations does the paper acknowledge?\n",
      "üß† Final Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "üß† Flexible Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters such as learning rate, batch size, number of iterations, and dropout rate for the pretext task. This decision, while practical for the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n",
      "\n",
      "üîí Strict Answer:\n",
      " The paper acknowledges that due to the computational demands of training neural networks on large EEG datasets, they fixed the hyperparameters (e.g., learning rate, batch size, number of iterations, dropout rate) for the pretext task. This decision, made for practical reasons within the scope of the study, may limit the models' adaptability to different datasets and training conditions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Strict prompt\n",
    "strict_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant analyzing a scientific research paper.\n",
    "\n",
    "Your job is to answer questions using **only the context provided below**. Do not rely on outside knowledge or assumptions.\n",
    "\n",
    "Answer rules:\n",
    "- If the answer is clearly stated, quote or summarize it briefly.\n",
    "- If the question uses a term not found in the context (e.g., \"hypothesis\"), but a related term exists (e.g., \"objective\" or \"research question\"), use that instead and mention it.\n",
    "- If the answer is not found at all, say exactly: \"Not found in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "# Flexible: no override = LangChain's default QA prompt\n",
    "qa_chain_strict = load_qa_chain(llm, chain_type=\"stuff\", prompt=strict_prompt)\n",
    "qa_chain_flexible = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "\n",
    "\n",
    "# Questions to test\n",
    "questions = [\n",
    "    \"What is the main goal of the research?\",\n",
    "    \"What is the main hypothesis or research question?\",\n",
    "    \"Summarize the findings of this study.\",\n",
    "    \"What are the main conclusions drawn in the paper?\",\n",
    "    \"What data or participants were used in the study?\",\n",
    "    \"What methods or models were applied?\",\n",
    "    \"What is Self-Supervised Learning in the context of this research?\",\n",
    "    \"Did the SSL model outperform the baseline?\",\n",
    "    \"How is this research relevant to autism?\",\n",
    "    \"What limitations does the paper acknowledge?\"\n",
    "]\n",
    "\n",
    "# Compare answers\n",
    "for i, query in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîπ Question {i}: {query}\")\n",
    "\n",
    "    # Retrieve chunks\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "    # Optional: print retrieved text for debugging\n",
    "    # for doc in docs:\n",
    "    #     print(doc.page_content[:300])\n",
    "    #     print(\"---\")\n",
    "\n",
    "    # Flexible chain\n",
    "    ans_flexible = qa_chain_flexible.run(input_documents=docs, question=query)\n",
    "\n",
    "    # Strict chain\n",
    "    # Run your normal QA chain first\n",
    "    answer = qa_chain_strict.run(input_documents=docs, question=query)\n",
    "\n",
    "# Trigger fallback if answer is weak or missing\n",
    "    if \"Not found in the provided context\" in answer or len(answer.strip()) < 10:\n",
    "        print(\"‚ö†Ô∏è Using fallback context...\")\n",
    "\n",
    "    # Use only intro + conclusion chunks for fallback\n",
    "        fallback_chunks = [\n",
    "            doc for doc in split_docs\n",
    "            if \"introduction\" in doc.page_content.lower() or \"conclusion\" in doc.page_content.lower()\n",
    "        ]\n",
    "        fallback_context = \"\\n\\n\".join([doc.page_content for doc in fallback_chunks])\n",
    "\n",
    "        # Run the fallback chain\n",
    "        answer = fallback_chain.run({\n",
    "            \"context\": fallback_context,\n",
    "            \"question\": query\n",
    "        })\n",
    "\n",
    "    print(\"üß† Final Answer:\\n\", answer)\n",
    "\n",
    "\n",
    "    # Output both\n",
    "    print(\"\\nüß† Flexible Answer:\\n\", ans_flexible)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
