{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"documents/TWDpdf.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#print(docs[0].page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total chunks created: 175\n",
      "\n",
      "ðŸ§© First chunk preview:\n",
      "1 \n",
      "Towards Multi-Brain Decoding in Autism:  \n",
      "A Self-Supervised Learning Approach \n",
      " \n",
      "Ghazaleh Ranjabaran1, Quentin Moreau1, Adrien Dubois1, Guillaume \n",
      "Dumas*1,2, \n",
      " \n",
      "1CHU Sainte-Justine Research Centre, Department of Psychiatry, UniversitÃ© de MontrÃ©al, \n",
      "MontrÃ©al, QC, Canada \n",
      "2Mila â€“ Quebec AI Institut...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,         # size of each chunk\n",
    "    chunk_overlap=100       # how much each chunk overlaps with the previous one\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Inspect the result\n",
    "print(f\"âœ… Total chunks created: {len(split_docs)}\")\n",
    "print(f\"\\nðŸ§© First chunk preview:\\n{split_docs[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\")[:8])  # Should show \"sk-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qazal\\AppData\\Local\\Temp\\ipykernel_18696\\254662876.py:9: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding model initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# (You wonâ€™t see any output yet â€” embeddings will be used in the next step)\n",
    "print(\"âœ… Embedding model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromaDB is a lightweight, local vector database â€” it's used to:\n",
    "\n",
    "- Store text embeddings (numeric vectors)\n",
    "\n",
    "- Search for similar vectors based on user queries\n",
    "\n",
    "- Power semantic search in RAG systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings stored in ChromaDB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qazal\\AppData\\Local\\Temp\\ipykernel_18696\\1080085975.py:11: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create Chroma DB and store the embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "# Save the DB to disk\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"âœ… Embeddings stored in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Result 1:\n",
      "downstream tasks. In the initial pretext task phase, the model is presented with a set of self -\n",
      "generated challe nges or auxiliary objectives. These challenges are carefully designed to \n",
      "encourage the model to extract meaningful and informative features from the unlabeled data. The \n",
      "model learns to uncover patterns, relationships, and representations within the data it self, \n",
      "effectively transforming it into a more structured and informative format....\n",
      "\n",
      "\n",
      "ðŸ”Ž Result 2:\n",
      "In this baseline approach, no pretext training is conducted, meaning the model starts from scratch \n",
      "without any learned representations from the SSL phase. The model is directly trained on the \n",
      "downstream task using the BBC2 dataset, with no prior knowledge or weight adjustments. This \n",
      "comparison evaluates whether the SSL pre -training adds value, as it allows us to assess if the \n",
      "model benefits from prior learning or if training from scratch performs similarly. The goal is to...\n",
      "\n",
      "\n",
      "ðŸ”Ž Result 3:\n",
      "https://doi.org/10.1038/s41572-019-0138-4 \n",
      "Matusz, P. J., Dikker, S., Huth, A. G., & Perrodin, C. (2019). Are We Ready for Real-world \n",
      "Neuroscience? Journal of Cognitive Neuroscience, 31(3), 327â€“338....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main goal of the research?\"\n",
    "results = vectorstore.similarity_search(query, k=3)  # return top 3 relevant chunks\n",
    "\n",
    "# Display the results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nðŸ”Ž Result {i+1}:\\n{res.page_content[:500]}...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
